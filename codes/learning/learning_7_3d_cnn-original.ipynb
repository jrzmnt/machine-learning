{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from mpl_toolkits.mplot3d import Axes3D # for 3d plotting\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_config(gpu):\n",
    "        print('Configuring environment')\n",
    "\n",
    "        tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "        #tf.reset_default_graph()\n",
    "\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "with h5py.File('../data/full_dataset_vectors.h5', 'r') as hf:\n",
    "    x_train_raw = hf[\"X_train\"][:]\n",
    "    y_train_raw = hf[\"y_train\"][:]\n",
    "    x_test_raw = hf[\"X_test\"][:]\n",
    "    y_test_raw = hf[\"y_test\"][:]\n",
    "\n",
    "\n",
    "# length check\n",
    "assert(len(x_train_raw) == len(y_train_raw))\n",
    "assert(len(x_test_raw) == len(y_test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D vector to rgb values, provided by ../input/plot3d.py\n",
    "def array_to_color(array, cmap=\"Oranges\"):\n",
    "    s_m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    return s_m.to_rgba(array)[:,:-1]\n",
    "\n",
    "# Transform data from 1d to 3d rgb\n",
    "def rgb_data_transform(data):\n",
    "    data_t = []\n",
    "    for i in range(data.shape[0]):\n",
    "        data_t.append(array_to_color(data[i]).reshape(16, 16, 16, 3))\n",
    "    return np.asarray(data_t, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:(10000, 2, 16, 16, 3)\n",
      "x_test:(2000, 2, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10 # from 0 to 9, 10 labels totally\n",
    "\n",
    "x_train = rgb_data_transform(x_train_raw)\n",
    "x_test = rgb_data_transform(x_test_raw)\n",
    "\n",
    "y_train = to_categorical(y_train_raw, n_classes)\n",
    "y_test = to_categorical(y_test_raw, n_classes)\n",
    "\n",
    "\n",
    "x_train = x_train[:,0:2,:,:]\n",
    "print('x_train:'+str(x_train.shape))\n",
    "\n",
    "x_test = x_test[:,0:2,:,:]\n",
    "print('x_test:'+str(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(x_train_data, keep_rate=0.7, seed=None):\n",
    "    \n",
    "    with tf.name_scope(\"layer_a\"):\n",
    "        # conv => 16*16*16\n",
    "        conv1 = tf.layers.conv3d(inputs=x_train_data, filters=16, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "        # conv => 16*16*16\n",
    "        conv2 = tf.layers.conv3d(inputs=conv1, filters=32, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "        # pool => 8*8*8\n",
    "        pool3 = tf.layers.max_pooling3d(inputs=conv2, pool_size=[2, 2, 2], strides=2)\n",
    "        \n",
    "    with tf.name_scope(\"layer_c\"):\n",
    "        # conv => 8*8*8\n",
    "        conv4 = tf.layers.conv3d(inputs=pool3, filters=64, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "        # conv => 8*8*8\n",
    "        conv5 = tf.layers.conv3d(inputs=conv4, filters=128, kernel_size=[3,3,3], padding='same', activation=tf.nn.relu)\n",
    "        # pool => 4*4*4\n",
    "        #pool6 = tf.layers.max_pooling3d(inputs=conv5, pool_size=[2, 2, 2], strides=2)\n",
    "        \n",
    "        \n",
    "    with tf.name_scope(\"batch_norm\"):\n",
    "        cnn3d_bn = tf.layers.batch_normalization(inputs=conv5, training=True)\n",
    "        \n",
    "    with tf.name_scope(\"fully_con\"):\n",
    "        flattening = tf.layers.flatten(cnn3d_bn)#tf.reshape(cnn3d_bn, [-1, 4*4*4*128])\n",
    "        dense = tf.layers.dense(inputs=flattening, units=128, activation=tf.nn.relu)\n",
    "        # (1-keep_rate) is the probability that the node will be kept\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=keep_rate, training=True)\n",
    "        \n",
    "    with tf.name_scope(\"y_conv\"):\n",
    "        y_conv = tf.layers.dense(inputs=dropout, units=10)\n",
    "    \n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x_train_data, y_train_data, x_test_data, y_test_data, learning_rate=0.05, keep_rate=0.7, epochs=10, batch_size=128):\n",
    "\n",
    "    gpu_number = 0\n",
    "    start_config(gpu_number)\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        prediction = cnn_model(x_input, keep_rate, seed=1)\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y_input))\n",
    "        saver = tf.train.Saver()\n",
    "                              \n",
    "    with tf.name_scope(\"training\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "           \n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_input, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    \n",
    "    iterations = int(len(x_train_data)/batch_size) + 1\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "    #with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        import datetime\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        iterations = int(len(x_train_data)/batch_size) + 1\n",
    "        # run epochs\n",
    "        for epoch in range(epochs):\n",
    "            start_time_epoch = datetime.datetime.now()\n",
    "            print('Epoch', epoch, 'started', end='')\n",
    "            epoch_loss = 0\n",
    "            # mini batch\n",
    "            for itr in range(iterations):\n",
    "                mini_batch_x = x_train_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                mini_batch_y = y_train_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                _optimizer, _cost = sess.run([optimizer, cost], feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "                epoch_loss += _cost\n",
    "\n",
    "            #  using mini batch in case not enough memory\n",
    "            acc = 0\n",
    "            itrs = int(len(x_test_data)/batch_size) + 1\n",
    "            for itr in range(itrs):\n",
    "                mini_batch_x_test = x_test_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                mini_batch_y_test = y_test_data[itr*batch_size: (itr+1)*batch_size]\n",
    "                acc += sess.run(accuracy, feed_dict={x_input: mini_batch_x_test, y_input: mini_batch_y_test})\n",
    "\n",
    "            end_time_epoch = datetime.datetime.now()\n",
    "            print(' Testing Set Accuracy:',acc/itrs, ' Time elapse: ', str(end_time_epoch - start_time_epoch))\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "\n",
    "        print('Time elapse: ', str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('inputs'):\n",
    "    x_input = tf.placeholder(tf.float32, shape=[None, 2, 224, 224, 3])\n",
    "    y_input = tf.placeholder(tf.float32, shape=[None, n_classes]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2, 224, 224, 3)\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train_temp = []\n",
    "for i in range(100):\n",
    "    x_train_temp.append(np.random.rand(2,224,224,3))\n",
    "    \n",
    "x_train_temp = np.asarray(x_train_temp)\n",
    "print(x_train_temp.shape)\n",
    "\n",
    "print(y_train[:10].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring environment\n",
      "Epoch 0 started Testing Set Accuracy: 0.09375  Time elapse:  0:00:04.155382\n",
      "Epoch 1 started Testing Set Accuracy: 0.0546875  Time elapse:  0:00:01.466104\n",
      "Epoch 2 started Testing Set Accuracy: 0.0390625  Time elapse:  0:00:01.465973\n",
      "Epoch 3 started Testing Set Accuracy: 0.0703125  Time elapse:  0:00:01.477866\n",
      "Epoch 4 started Testing Set Accuracy: 0.0390625  Time elapse:  0:00:01.468858\n",
      "Epoch 5 started Testing Set Accuracy: 0.078125  Time elapse:  0:00:01.469577\n",
      "Epoch 6 started Testing Set Accuracy: 0.0625  Time elapse:  0:00:01.469011\n",
      "Epoch 7 started Testing Set Accuracy: 0.0625  Time elapse:  0:00:01.472653\n",
      "Epoch 8 started Testing Set Accuracy: 0.0546875  Time elapse:  0:00:01.476749\n",
      "Epoch 9 started Testing Set Accuracy: 0.0703125  Time elapse:  0:00:01.476717\n",
      "Time elapse:  0:00:17.401226\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x_train_temp, \n",
    "                     y_train[:100], \n",
    "                     x_train_temp[:100], \n",
    "                     y_test[:100], \n",
    "                     epochs=10, \n",
    "                     batch_size=32, \n",
    "                     learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2, 16, 16, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
