{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let’s Gym Together\n",
    "What is OpenAI gym ? This python library gives us huge number of test environments to work on our RL agent’s algorithms with shared interfaces for writing general algorithms and testing them. Let’s get started just type pip install gym on terminal for easy install, you’ll get some classic environment to start working on your agent. Copy the code below and run it, your environment will get loaded only classic control comes as default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. It renders instance for 300 timesteps, perform random actions\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.reset()\n",
    "\n",
    "for _ in range(300):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. To check all env available, uninstalled ones are also shown\n",
    "\n",
    "envs = gym.envs.registry.all()\n",
    "len(envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When object interacts with environment with an action then step(…) function returns ```observation``` which represents environments state, ```reward``` a float of reward in previous action, ```done``` when its time to reset the environment or goal achieved and ```info``` a dict for debugging, it can be used for learning if it contains raw probabilities of environment’s last state. See how it works. Also, observe how ```observation``` of type ```Space``` is different for different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0') # try for different environements\n",
    "observation = env.reset()\n",
    "\n",
    "for t in range(200):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is ```action_space``` in above code? ```action-space``` & ```observation-space``` describes what is the valid format for that particular env to work on with. Just take a look at values returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space) #[Output: ] Discrete(2)\n",
    "print(env.observation_space) # [Output: ] Box(4,)\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "print(env.action_space) #[Output: ] Box(1,)\n",
    "print(env.observation_space) #[Output: ] Box(2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete is non-negative possible values, above 0 or 1 are equivalent to left and right movement for CartPole balancing. Box represent n-dim array. These can help in writing general codes for different environments. As we can simply check the bounds ```env.observation_space.high/[low]``` and code them into our general algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Illustration\n",
    "\n",
    "I’ll recommend after knowing basics of OpenAI’s gym you can install all dependencies of gym and then completely install gym with following commands. Here, we are using python2.x you can also use python3.x just change below commands for it accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- apt-get install \n",
    "   - python-numpy \n",
    "   - python-dev \n",
    "   - cmake \n",
    "   - zlib1g-dev \n",
    "   - libjpeg-dev \n",
    "   - xvfb \n",
    "   - libav-tools \n",
    "   - xorg-dev \n",
    "   - python-opengl -libboost-all-dev \n",
    "   - libsdl2-dev swig \n",
    "\n",
    "- sudo pip install 'gym[all]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start building our Q-table algorithm, which will try to solve [FrozenLake environment](https://gym.openai.com/envs/FrozenLake8x8-v0/). In this environment aim is to reach the goal, on a frozen lake that might have some holes in it. Here is how surface is depicted by this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q table contains state-action pairs mapping to reward. So, we will construct an array which maps different state and actions to reward values during run of algorithm. Its dimension will clearly |states|x|actions|. Let’s write it in code for Q-learning Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Environment and Q-table structure\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "\n",
    "# env.observation.n, env.action_space.n gives number of states and action in env loaded\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Parameters of Q-leanring\n",
    "eta = .628\n",
    "gma = .9\n",
    "epis = 500\n",
    "rev_list = [] # rewards per episode calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Q-learning Algorithm\n",
    "for i in range(epis):\n",
    "    # Reset environment\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        env.render()\n",
    "        j+=1\n",
    "        # Choose action from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state & reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + eta*(r + gma*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    rev_list.append(rAll)\n",
    "    env.render()\n",
    "    env.close()\n",
    "    \n",
    "print(\"Reward Sum on all episodes \" + str(sum(rev_list)/epis))\n",
    "print(\"Final Values Q-Table\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in simulation of agent to find the solution through the environment write this snippet instead of Q-learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://miro.medium.com/max/650/1*S6CG3jyp5rGxMUGw_Bqr3Q.png\"/>\n",
    "<br />\n",
    "    <i>Frozen Lake Environment’s Visualization & Below code is for its simulation.</i>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "s = env.reset()\n",
    "d = False\n",
    "# The Q-Table learning algorithm\n",
    "while d != True:\n",
    "    env.render()\n",
    "    # Choose action from Q table\n",
    "    a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "    #Get new state & reward from environment\n",
    "    s1,r,d,_ = env.step(a)\n",
    "    #Update Q-Table with new knowledge\n",
    "    Q[s,a] = Q[s,a] + eta*(r + gma*np.max(Q[s1,:]) - Q[s,a])\n",
    "    s = s1\n",
    "# Code will stop at d == True, and render one state before it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But do remember even with common interface the code complexity will be different for different environments. In above environment we only had a simple 64 state environment only with few actions only to handle. We were able to store them in two dimensional array for reward mapping very easily. Now, Let’s consider more complicated environment case like *Atari envs* and look at the approach that is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v0\")\n",
    "\n",
    "# action_space\n",
    "print(env.action_space.n)\n",
    "\n",
    "# env.get_action_meanings\n",
    "print(env.env.get_action_meanings())\n",
    "\n",
    "# env.observation()\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```observation_space``` is needed to be represented by 210x160x3 tensor which makes our Q-table even more complicated. Also, each action is repeatedly performed for a duration of k frames, where k is uniformly sampled from {2,3,4}. With 33,600 pixels in RGB channels with values ranging from 0–255 the environment clearly has become over complicated simple Q-learning approach can’t be used here. Deep learning with its CNN architecture is the solution for this problem and topic for follow up of this introductory article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Now, with the above tutorial you have the basic knowledge about the gym and all you need to get started with it. Gym is also TensorFlow compatible but I haven’t used it to keep the tutorial simple. After trying out gym you must get started with [baselines](https://github.com/openai/baselines) for good implementations of RL algorithms to compare your implementations. To see all the OpenAI tools check out their [github page](https://github.com/openai). RL is an expanding fields with applications in huge number of domains and it will play an important role in future AI breakthroughs. Thanks for reading!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
